**Overview**

This repository explores Word Embeddings, a popular technique in natural language processing (NLP) for mapping words to dense vector representations. Word embeddings capture semantic relationships between words, allowing models to understand meaning and context in language. The repository demonstrates various methods for training and using word embeddings, including classical methods like Word2Vec, GloVe, and more recent techniques like FastText.

**Key Features**

- **Pretrained Word Embeddings:** Includes pretrained embeddings for efficient use in NLP tasks.
- **Word2Vec, GloVe, FastText:** Demonstrates how to train and utilize these embeddings for different NLP tasks.
- **Visualization:** Includes visual representations of word embeddings to understand semantic relationships.
- **Custom Embedding Training:** Allows you to train embeddings on your own datasets for domain-specific tasks.
